{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/karfly/learning-deep-learning/blob/master/04_dense/seminar_photo2map.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsqQ3iSpVL7i"
   },
   "source": [
    "# Photo -> Map\n",
    "Disclaimer: this notebook is an adapted version of [this repository](https://github.com/GunhoChoi/Kind-PyTorch-Tutorial).\n",
    "\n",
    "Previously, we used neural networks for **sparse** predictions: large input (image) -> small output (vector with 10 elements, e.g. CIFAR10 classes). Today we'll use deep learning to make **dense** predictions (large input (image) -> large output (image)) for **image-to-image translation problem**. *Image-to-image translation* is a wide class of problems, where input is image and output is image too (e.g. satellite photo -> map, image stylization, [sketch -> cat portrait](https://affinelayer.com/pixsrv/),  etc...). There many good models for dense predictions, but we'll use **UNet** as the best choice in terms of simplicity-quality ratio.\n",
    "\n",
    "But before we start, let's look at our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwBjoCGoVL7f"
   },
   "source": [
    "<img src=\"https://github.com/karfly/learning-deep-learning/blob/master/04_dense/static/photo2map.jpg?raw=true\" width=700 align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Dataset\n",
    "We'll create a dataset of pairs **satellite photo - map** (example is above). To download dataset, uncomment and execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/maps.tar.gz\n",
    "# ! tar -xzvf maps.tar.gz\n",
    "# ! mkdir maps/train/0 && mv maps/train/*.jpg maps/train/0\n",
    "# ! mkdir maps/val/0 && mv maps/val/*.jpg maps/val/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm    # run this in Colab\n",
    "from tqdm import tqdm               # or this in Jupyter instead\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_title = \"unet\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 4\n",
    "image_size = 256\n",
    "\n",
    "data_dir = \"./maps\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),    # converts input image to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading and unpacking you'll find directory `maps` with 2 subdirectories: `train` and `val`. Each image is a pair (photo - map), so we'll have to **crop image to obtain input and target**. Let's use PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder) dataloader: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw sample from dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange    # do \"!pip install einops\" in case einops is not installed\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize to [0, 1]\n",
    "    img = img.cpu().numpy()\n",
    "    \n",
    "#     img_reshaped = np.transpose(img, (1, 2, 0))     # torch returns an image of shape (3, H, W), you need to convert it to (H, W, 3)\n",
    "\n",
    "    # instead, below we use einops.rearrange. You can read about this neat library here: https://github.com/arogozhnikov/einops/blob/master/docs/1-einops-basics.ipynb\n",
    "    img_reshaped = rearrange(img, 'c h w -> h w c')\n",
    "    \n",
    "    plt.imshow(img_reshaped)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(image, _) = train_dataset[0]\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see input and target are in the same image. Let's write wrapper of ImageFolder to return what we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoMapDataset(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, _ = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        # sample will be of shape (3, H, W)\n",
    "        \n",
    "        photo_image, map_image = # your code here \n",
    "        \n",
    "        return photo_image, map_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PhotoMapDataset(root=os.path.join(data_dir, \"train\"), transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_image, map_image = train_dataset[0]\n",
    "imshow(photo_image)\n",
    "imshow(map_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we're done with data. Let's move to defining model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. U-Net\n",
    "UNet is a very popular fully-convolutional architecture. Below you can find its structure (for more details refer to [original paper](https://arxiv.org/abs/1505.04597)):\n",
    "\n",
    "<!-- <img src=\"https://github.com/karfly/learning-deep-learning/blob/master/04_dense/static/unet.png?raw=true\" width=800 align=\"center\"/> -->\n",
    "\n",
    "<img src=\"https://i.imgur.com/z0ceKX1.png\" width=800 align=\"center\"/>\n",
    "\n",
    "Let's build U-Net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNjNVQgzfIky"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNetDownBlock: Conv + ReLU + Conv + ReLU [+ MaxPool]\n",
    "\n",
    "class UnetDownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pooling=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        return x, x_before_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNetUpBlock: upsampling + concat + Conv + ReLU\n",
    "\n",
    "class UnetUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    def forward(self, x, x_bridge):\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, depth=3, base_n_filters=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        message = '{}(in_channels={}, out_channels={}, depth={}, base_n_filters={})'.format(\n",
    "            self.__class__.__name__,\n",
    "            self.in_channels, self.out_channels, self.depth, self.base_n_filters\n",
    "        )\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(3, 3, depth=4, base_n_filters=64).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up SummaryWriter for TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "experiment_name = \"{}@{}\".format(experiment_title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "print('experiment_name:', experiment_name)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"./tb\", experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to create valid_dataset and valid_dataloader here (see Task 3 below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    n_iters = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # unpack batch\n",
    "        photo_image_batch, map_image_batch = batch\n",
    "        photo_image_batch, map_image_batch = photo_image_batch.to(device), map_image_batch.to(device)\n",
    "        \n",
    "        # forward\n",
    "        map_image_pred_batch = model(photo_image_batch)\n",
    "\n",
    "        loss = criterion(map_image_pred_batch, map_image_batch)\n",
    "        \n",
    "        # optimize\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # dump statistics\n",
    "        writer.add_scalar(\"train/loss\", loss.item(), global_step=epoch * len(train_dataloader) + n_iters)\n",
    "        \n",
    "        if n_iters % 50 == 0:\n",
    "            writer.add_image('train/photo_image', torchvision.utils.make_grid(photo_image_batch) * 0.5 + 0.5, \n",
    "                             epoch * len(train_dataloader) + n_iters)\n",
    "            writer.add_image('train/map_image_pred', torchvision.utils.make_grid(map_image_pred_batch), \n",
    "                             epoch * len(train_dataloader) + n_iters)\n",
    "            writer.add_image('train/map_image_gt', torchvision.utils.make_grid(map_image_batch), \n",
    "                             epoch * len(train_dataloader) + n_iters)\n",
    "        \n",
    "        n_iters += 1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    print(\"Epoch {} done.\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard\n",
    "\n",
    "To look at your logs in tensorboard go to terminal and run command:\n",
    "```bash\n",
    "$ tensorboard --logdir PATH_TO_YOUR_LOG_DIR\n",
    "```\n",
    "\n",
    "Then go to browser to `localhost:6006` and you'll see beautiful graphs! Always use tensorbord to watch your experiment, because it's very important to check how training is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running in Colab, uncomment the following cells instead to initialize TensorBoard in Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs_dir = os.path.join(\"./tb\", experiment_name)\n",
    "# %tensorboard --logdir {logs_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f4Qol840VL8a"
   },
   "source": [
    "## Task 3. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kno_Wj27VL8a"
   },
   "source": [
    "As you remember we have `val` images in our dataset. So, to make sure, that we didn't overfit to `train`, we should do evaluation on validation set. You're free to choose, how to insert validation in existing notebook:\n",
    "1. Insert validation to train-loop (validate every epoch)\n",
    "2. Validate 1 time after training\n",
    "\n",
    "I highly recomend to implement first option with beautiful tensorboard logs. Have fun! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
