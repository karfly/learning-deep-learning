{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seminar_salary_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gwM9DB-om_6",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karfly/learning-deep-learning/blob/master/07_emb/seminar_salary_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvlEFroaom_9",
        "colab_type": "text"
      },
      "source": [
        "# Salary prediction from job description\n",
        "\n",
        "Today we're gonna apply the newly learned DL tools for sequence processing to the task of predicting job salary.\n",
        "\n",
        "Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the core of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3QJGxumom__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import trange\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od8_g4oPonAH",
        "colab_type": "text"
      },
      "source": [
        "## About the challenge\n",
        "\n",
        "\n",
        "\n",
        "Our task is to predict one number, __SalaryNormalized__, in the sense of minimizing __Mean Absolute Error__.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/karfly/learning-deep-learning/master/07_emb/static/salary-prediction.png\" width=400px>\n",
        "\n",
        "\n",
        "\n",
        "To do so, our model will access a number of features:\n",
        "* Natural language text: __`Title`__ and  __`FullDescription`__\n",
        "* Categorical features: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__.\n",
        "\n",
        "\n",
        "You can read more [in the official description](https://www.kaggle.com/c/job-salary-prediction#description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62sbRFXFonAH",
        "colab_type": "text"
      },
      "source": [
        "## Download data\n",
        "\n",
        "If for any reason data downloading failed (Dropbox quota can exceed):\n",
        "- you can manually download it from [here](https://www.dropbox.com/s/uni730jpy90eh6f/Train_rev1.csv.tar?dl=0)\n",
        "- get it from the original [Kaggle competition page](https://www.kaggle.com/c/job-salary-prediction/data) (download `Train_rev1.*`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjsdAU2onAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -L https://www.dropbox.com/s/uni730jpy90eh6f/Train_rev1.csv.tar?dl=! -o Train_rev1.csv.tar\n",
        "!tar -xvf ./Train_rev1.csv.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CWDdqFFonAP",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Neural networks don't know anything about *words*, but they do know about *tensors*. In this section we'll preprocess our data, build an **inverse token index** (token -> some unique index) and present it in the form which can be successfully fed to the neural network.\n",
        "\n",
        "We'll do the following steps:\n",
        "1. Load data\n",
        "2. Tokenize text features\n",
        "3. Wipe out rarely occured tokens\n",
        "4. Add special tokens to the vocabulary (`PAD`, `UNK`)\n",
        "5. Build an inverse token index\n",
        "6. Implement function for converting tokens to tensors\n",
        "7. Preprocess categorical features\n",
        "8. Split data in train/test "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi46XR7uonAQ",
        "colab_type": "text"
      },
      "source": [
        "### 1. Load data\n",
        "\n",
        "As a target we'll use not pure $\\text{salary}$, but $\\log(1 + \\text{salary})$. It's a common trick in ML, because it makes a skewed target variable more normal (more [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/103975))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sArHBHuonAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
        "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
        "\n",
        "text_columns = ['Title', 'FullDescription']\n",
        "categorical_columns = ['Category', 'Company', 'LocationNormalized', 'ContractType', 'ContractTime']\n",
        "\n",
        "target_column = 'Log1pSalary'\n",
        "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast nan to string\n",
        "\n",
        "data.sample(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWy-uZ2IonAV",
        "colab_type": "text"
      },
      "source": [
        "### 2. Tokenize text features\n",
        "\n",
        "To even begin training our neural network, we'll need to preprocess the text features. Since it is not an NLP course, we're gonna use simple built-in NLTK tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLI5ue8HonAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpne-2VwonAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Before tokenization\")\n",
        "print(data['Title'][:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHTB0fSGonAd",
        "colab_type": "text"
      },
      "source": [
        "Here we tokenize text columns of our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVYvW9w-onAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "\n",
        "for col in text_columns:\n",
        "    data[col] = data[col].apply(lambda l: ' '.join(tokenizer.tokenize(str(l).lower())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nrt9aqLonAg",
        "colab_type": "text"
      },
      "source": [
        "Now we can assume that our text is a space-separated list of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnXoH3djonAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"After tokenization\")\n",
        "print(data['Title'][:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCLasrGsonAi",
        "colab_type": "text"
      },
      "source": [
        "### 3. Wipe out rarely occured tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCBy0a_8onAi",
        "colab_type": "text"
      },
      "source": [
        "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. Let's see how many times each word is present in the data so that we can build a \"white list\" of known words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrhmMPYAonAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "token_counts = Counter()\n",
        "\n",
        "# count how many times does each token occur in text columns\n",
        "for col in text_columns:\n",
        "    data[col].apply(lambda x: token_counts.update(x.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF8Xzb9donAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total unique tokens :\", len(token_counts))\n",
        "print(\"Most common:\", token_counts.most_common(n=5))\n",
        "print(\"Least common:\", token_counts.most_common()[-3:])\n",
        "\n",
        "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
        "assert len(token_counts) in range(200000, 210000)\n",
        "print(\"Correct!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM2M3JRTonAo",
        "colab_type": "text"
      },
      "source": [
        "Let's see how many words are there for each count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKL1ubnDonAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(list(token_counts.values()), range=[0, 10 ** 4], bins=50, log=True)\n",
        "plt.xlabel(\"Word counts\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEpi8AHoonAr",
        "colab_type": "text"
      },
      "source": [
        "**Fun fact**: such disdributions are very common in real world and there is a term for it – [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) (here's a nice [Vsauce video](https://www.youtube.com/watch?v=fCn8zs912OE) about it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il8fGuLconAr",
        "colab_type": "text"
      },
      "source": [
        "Get a list of all tokens that occur at least 10 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVZkIzRuonAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_count = 10\n",
        "\n",
        "# take only tokens that occured more than min_count times\n",
        "tokens = ## your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEaIrwq3onAt",
        "colab_type": "text"
      },
      "source": [
        "### 4. Add special tokens to the vocabulary (`PAD`, `UNK`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWMkdOo2onAu",
        "colab_type": "text"
      },
      "source": [
        "Add a special tokens for unknown and empty words. The purpose of adding these tokens will be more clear later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YhpbrlGonAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD, UNK = 'PAD', 'UNK'\n",
        "tokens = [PAD, UNK] + tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KPXrBK8onAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total number of tokens:\", len(tokens))\n",
        "\n",
        "assert type(tokens) == list\n",
        "assert len(tokens) in range(32000, 35000)\n",
        "assert 'me' in tokens\n",
        "assert UNK in tokens\n",
        "print(\"Correct!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyKmv357onAy",
        "colab_type": "text"
      },
      "source": [
        "### 5. Build an inverse token index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJOU8Q94onAy",
        "colab_type": "text"
      },
      "source": [
        "Build an inverse token index: a dictionary from token (string) to it's index (int) in `tokens` list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUaGZ5XmonAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_to_id = ## your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7OajyeEonA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert isinstance(token_to_id, dict)\n",
        "assert len(token_to_id) == len(tokens)\n",
        "for token in tokens:\n",
        "    assert tokens[token_to_id[token]] == token\n",
        "\n",
        "print(\"Correct!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBexiFhDonA1",
        "colab_type": "text"
      },
      "source": [
        "### 6. Implement function for converting tokens to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPPxumPnonA2",
        "colab_type": "text"
      },
      "source": [
        "Let's use the vocabulary you've built to map text lines into torch-digestible matrices. The sentences can be of different lengths, so we pad them with `PAD` token to the length of the longest sentence. We do it to be able to process batches of sentences, because tensors' rows **must** be of the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9k8c1URonA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_IX, UNK_IX = token_to_id[PAD], token_to_id[UNK]\n",
        "\n",
        "def as_matrix(sequences, max_len=None):\n",
        "    \"\"\"Convert a list of tokens into a matrix with padding\"\"\"\n",
        "    \n",
        "    if isinstance(sequences[0], str):\n",
        "        sequences = list(map(str.split, sequences))\n",
        "        \n",
        "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
        "    \n",
        "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
        "    for i, seq in enumerate(sequences):\n",
        "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
        "        matrix[i, :len(row_ix)] = row_ix\n",
        "    \n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJX8C23lonA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Lines:\")\n",
        "print('\\n'.join(data['Title'][:3].values))\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Matrix:\")\n",
        "print(as_matrix(data['Title'][:3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylNX5MISonA6",
        "colab_type": "text"
      },
      "source": [
        "### 7. Preprocess categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywK47BsDonA6",
        "colab_type": "text"
      },
      "source": [
        "Now let's  encode the categirical data we have. As usual, we'll use one-hot encoding for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YBsP3odonA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# we only consider top-1k most frequent companies to minimize memory usage\n",
        "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
        "recognized_companies = set(top_companies)\n",
        "data['Company'] = data['Company'].apply(lambda comp: comp if comp in recognized_companies else 'Other')\n",
        "\n",
        "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
        "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1tg8oxlonA8",
        "colab_type": "text"
      },
      "source": [
        "### 8. Split data in train/test \n",
        "\n",
        "Once we've learned to tokenize the data, let's design a machine learning experiment. As before, we won't focus too much on validation, opting for a simple train-test split.\n",
        "\n",
        "**To be completely rigorous,** we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hixae9oronA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_val = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Train size =\", len(data_train))\n",
        "print(\"Validation size =\", len(data_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpOs-HCeonA-",
        "colab_type": "text"
      },
      "source": [
        "## Building models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R024gBdionA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kSu043wonBA",
        "colab_type": "text"
      },
      "source": [
        "Let's first write a helper function for sampling batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9cupvZUonBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_batch(data, batch_size, device='cpu', max_len=None, replace=True):\n",
        "    \"\"\"Creates a pytorch-friendly dict from the batch data\"\"\"\n",
        "    \n",
        "    if batch_size is not None:\n",
        "        data = data.sample(batch_size, replace=replace)\n",
        "    \n",
        "    batch = {}\n",
        "    for col in text_columns:\n",
        "        batch[col] = torch.tensor(as_matrix(data[col].values, max_len), dtype=torch.long).to(device)\n",
        "    \n",
        "    batch['Categorical'] = torch.tensor(\n",
        "        categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1)), dtype=torch.float32\n",
        "    ).to(device)\n",
        "    \n",
        "    if target_column in data.columns:\n",
        "        batch[target_column] = torch.tensor(data[target_column].values, dtype=torch.float32).to(device)\n",
        "    \n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "740_M_RdonBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_batch(data_train, 2, max_len=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDMTaBwNonBE",
        "colab_type": "text"
      },
      "source": [
        "Out model consists of three branches:\n",
        "* Title encoder\n",
        "* Description encoder\n",
        "* Categorical features encoder\n",
        "\n",
        "We will then feed all 3 branches into one common network that predicts salary.\n",
        "\n",
        "![scheme](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/07_emb/static/salary-prediction-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z50tXeu4onBE",
        "colab_type": "text"
      },
      "source": [
        "By default, both text vectorizers will use 1d convolutions, followed by global pooling over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-9vaFIvonBE",
        "colab_type": "text"
      },
      "source": [
        "### GlobalMaxPooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsTcJ5ZZonBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GlobalMaxPooling(nn.Module):\n",
        "    def __init__(self, dim=-1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x.max(dim=self.dim)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ2iOQLhonBG",
        "colab_type": "text"
      },
      "source": [
        "### TitleEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hZLOEI-onBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TitleEncoder(nn.Module):\n",
        "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
        "        \"\"\" \n",
        "        A simple sequential encoder for titles.\n",
        "        x -> emb -> conv -> global_max -> relu -> dense\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # here `padding_idx` just inits padding embedding with zeros\n",
        "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)  \n",
        "        self.conv = nn.Conv1d(64, out_size, kernel_size=3, padding=1)\n",
        "        self.pool = GlobalMaxPooling()\n",
        "        self.dense = nn.Linear(out_size, out_size)\n",
        "\n",
        "    def forward(self, text_ix):\n",
        "        \"\"\"\n",
        "        :param text_ix: int64 tensor of shape [batch_size, max_len]\n",
        "        :returns: float32 tensor of shape [batch_size, out_size]\n",
        "        \"\"\"\n",
        "        x = self.emb(text_ix)\n",
        "\n",
        "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
        "        x = torch.transpose(x, 1, 2)\n",
        "        \n",
        "        # apply the layers as defined above (don't forget about nonlinearity)\n",
        "        \n",
        "        ## your code herer\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9_sDmLvonBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_encoder = TitleEncoder(out_size=64).to(device)\n",
        "\n",
        "batch = sample_batch(data_train, 4, device=device)\n",
        "prediction = title_encoder(batch['Title'])\n",
        "\n",
        "assert tuple(prediction.shape) == (batch['Title'].shape[0], 64)\n",
        "\n",
        "del title_encoder\n",
        "print(\"Seems fine\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j50Q45IzonBJ",
        "colab_type": "text"
      },
      "source": [
        "### DescriptionEncoder\n",
        "\n",
        "It has pretty the same architecture as `TitleEncoder`, but intuitively it seems that `description` of the job is a more complex text, than `title`. So let's make it deeper (more conv and dense layers) and wider (bigger embedding space and bigger intermediate channels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VyyT_ZponBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DescriptionEncoder(nn.Module):\n",
        "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
        "        \"\"\" \n",
        "        A simple sequential encoder for titles.\n",
        "        x -> emb -> conv -> relu -> conv -> global_max -> relu -> dense -> relu -> dense\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        ## your code here\n",
        "\n",
        "    def forward(self, text_ix):\n",
        "        \"\"\"\n",
        "        :param text_ix: int64 tensor of shape [batch_size, max_len]\n",
        "        :returns: float32 tensor of shape [batch_size, out_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        ## your code here\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5YomHOJonBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "description_encoder = DescriptionEncoder(out_size=64).to(device)\n",
        "\n",
        "batch = sample_batch(data_train, 4, device=device)\n",
        "prediction = description_encoder(batch['FullDescription'])\n",
        "\n",
        "assert tuple(prediction.shape) == (batch['FullDescription'].shape[0], 64)\n",
        "\n",
        "del description_encoder\n",
        "print(\"Seems fine too\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTCC3CHuonBM",
        "colab_type": "text"
      },
      "source": [
        "### CategoricalEncoder\n",
        "\n",
        "Let's build simple dense network for encoding categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV5L8vYLonBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CategoricalEncoder(nn.Module):\n",
        "    def __init__(self, n_categorical_features=len(categorical_vectorizer.vocabulary_), out_size=64):\n",
        "        \"\"\" \n",
        "        A simple dense encoder for categorical features.\n",
        "        x -> dense -> relu -> dense\n",
        "        \"\"\"\n",
        "        super().__init__()  \n",
        "        \n",
        "        self.dense_1 = nn.Linear(n_categorical_features, 64)\n",
        "        self.dense_2 = nn.Linear(64, out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: float32 tensor of shape [batch_size, n_categorical_features]\n",
        "        :returns: float32 tensor of shape [batch_size, out_size]\n",
        "        \"\"\"\n",
        "        x = self.dense_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dense_2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6TcMREsonBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_encoder = CategoricalEncoder(out_size=64).to(device)\n",
        "\n",
        "batch = sample_batch(data_train, 4, device=device)\n",
        "prediction = categorical_encoder(batch['Categorical'])\n",
        "\n",
        "assert tuple(prediction.shape) == (batch['Categorical'].shape[0], 64)\n",
        "\n",
        "del categorical_encoder\n",
        "print(\"And this is fine... wow\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oko8z1b3onBO",
        "colab_type": "text"
      },
      "source": [
        "### One network ~~to rule them all~~\n",
        "\n",
        "Takes `title_ix`, `description_ix`, `categorical_features` as input, encodes them with corresponding encoders, concatenates encoded vectors and feed to simple dense network which predicts log1salary (1 scalar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD4MThbQonBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SalaryPredictionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
        "    It unites title & description encoders you defined above as long as some layers for head and categorical branch.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_tokens=len(tokens), n_categorical_features=len(categorical_vectorizer.vocabulary_)):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.title_encoder = TitleEncoder(out_size=64)\n",
        "        self.description_encoder = DescriptionEncoder(out_size=64)\n",
        "        self.categorical_encoder = CategoricalEncoder(n_categorical_features=n_categorical_features, out_size=128)\n",
        "        \n",
        "        # define \"output\" layers that combines encoded features and predicts the answer\n",
        "        self.dense_1 = nn.Linear(64 + 64 + 128, 256)\n",
        "        self.dense_2 = nn.Linear(256, 64)\n",
        "        self.dense_3 = nn.Linear(64, 1)\n",
        "        \n",
        "        \n",
        "    def forward(self, title_ix, description_ix, categorical_features):\n",
        "        \"\"\"\n",
        "        :param title_ix: int32 tensor of shape [batch, title_len], job titles encoded by as_matrix\n",
        "        :param description_ix:  int32 tensor of shape [batch, desc_len] , job descriptions encoded by as_matrix\n",
        "        :param categorical_features: float32 tensor of shape [batch, n_cat_features]\n",
        "        :returns: float32 1d tensor [batch], predicted log1p-salary\n",
        "        \"\"\"\n",
        "        \n",
        "        # process each data source with it's respective encoder\n",
        "        title_x = self.title_encoder(title_ix)\n",
        "        description_x = self.description_encoder(description_ix)\n",
        "        categorical_x = self.categorical_encoder(categorical_features)\n",
        "        \n",
        "        # concatenate all vectors together...\n",
        "        joint_x = torch.cat([title_x, description_x, categorical_x], dim=1)\n",
        "        \n",
        "        # apply layers for processing the concatenated encoded vectors\n",
        "        \n",
        "        ## your code here\n",
        "        \n",
        "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
        "        # Note 2: please do not use output nonlinearities.\n",
        "        return joint_x[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8QZUd-qonBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SalaryPredictionNetwork().to(device)\n",
        "\n",
        "batch = sample_batch(data_train, 32, device=device)\n",
        "prediction = model(batch['Title'], batch['FullDescription'], batch['Categorical'])\n",
        "assert len(prediction.shape) == 1 and prediction.shape[0] == batch['Title'].shape[0]\n",
        "\n",
        "# check grads\n",
        "loss = prediction.norm()\n",
        "loss.backward()\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    grad = p.grad\n",
        "    assert grad is not None and not (grad == 0).all(), \\\n",
        "    f\"Some model parameters received zero grads ({name}). Double-check that your model uses all it's layers.\"\n",
        "\n",
        "print(\"The whole model seems fine. You're awesome!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_WDZZOPonBQ",
        "colab_type": "text"
      },
      "source": [
        "### Losses\n",
        "\n",
        "For loss we'll use `MSE`. As a metric we'll use `MAE`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R7StvOGonBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(reference, prediction):\n",
        "    \"\"\"\n",
        "    Computes objective for minimization.\n",
        "    By deafult we minimize MSE, but you are encouraged to try mix up MSE, MAE, huber loss, etc.\n",
        "    \"\"\"\n",
        "    loss = ## your MSE code here\n",
        "    return loss\n",
        "\n",
        "def compute_mae(reference, prediction):\n",
        "    \"\"\"Computes MAE on actual salary, assuming your model outputs log1p(salary)\"\"\"\n",
        "    return torch.abs(torch.exp(reference) - torch.exp(prediction)).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFXVICODonBS",
        "colab_type": "text"
      },
      "source": [
        "## Train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pk8lB6TonBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_minibatches(data, batch_size, device='cpu', max_len=None, max_batches=None, shuffle=True, verbose=False):\n",
        "    indices = np.arange(len(data))\n",
        "    \n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(indices)\n",
        "    if max_batches is not None:\n",
        "        indices = indices[:batch_size * max_batches]\n",
        "    \n",
        "    irange = trange if verbose else range\n",
        "    for start in irange(0, len(indices), batch_size):\n",
        "        yield sample_batch(data.iloc[indices[start : start + batch_size]], batch_size, device=device, max_len=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS2ub8QMonBT",
        "colab_type": "text"
      },
      "source": [
        "Training parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TJQhR_WonBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 100\n",
        "max_len = 100\n",
        "batch_size = 32\n",
        "batches_per_epoch = 100\n",
        "loss_visualization_window = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDJrBC_HonBU",
        "colab_type": "text"
      },
      "source": [
        "Setup model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C16AzDUronBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SalaryPredictionNetwork().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AsY-A3sonBW",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's train our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTSiKy6fonB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch_i in range(num_epochs):    \n",
        "    # train\n",
        "    train_loss = train_mae = train_batches = 0    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterate_minibatches(data_train, batch_size, device=device, max_batches=batches_per_epoch):\n",
        "        prediction = ## make a forward pass for the model\n",
        "        reference = batch[target_column]\n",
        "\n",
        "        loss = compute_loss(reference, prediction)\n",
        "        \n",
        "        ## your optimization step here (remember the mantra? zero-backward-step, zero-backward-step, ...)\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        train_losses.append(loss.item())\n",
        "        \n",
        "        train_mae += compute_mae(reference, prediction).item()\n",
        "        train_batches += 1\n",
        "    \n",
        "    # val\n",
        "    val_loss = val_mae = val_batches = 0\n",
        "    model.eval()\n",
        "    \n",
        "    for batch in iterate_minibatches(data_val, batch_size, shuffle=False, device=device):\n",
        "        prediction = ## make a forward pass for the model\n",
        "        reference = batch[target_column]\n",
        "        \n",
        "        loss = compute_loss(reference, prediction)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        val_losses.append(loss.item())\n",
        "        \n",
        "        val_mae += compute_mae(reference, prediction).item()\n",
        "        val_batches += 1\n",
        "    \n",
        "    # visualization\n",
        "    clear_output(True)\n",
        "    plt.plot(\n",
        "        np.arange(len(train_losses))[-loss_visualization_window:],\n",
        "        train_losses[-loss_visualization_window:]\n",
        "    )\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Epoch {epoch_i}/{num_epochs}\")\n",
        "    print(\"train: loss={:.5f}, MAE={:.1f}\".format(train_loss / train_batches, train_mae / train_batches))\n",
        "    print(\"val: loss={:.5f}, MAE={:.1f}\".format(val_loss / val_batches, val_mae / val_batches))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iocobL_onB8",
        "colab_type": "text"
      },
      "source": [
        "After training let's make a final validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3QEazF2onB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "val_loss = val_mae = val_batches = 0\n",
        "for batch in iterate_minibatches(data_val, batch_size, shuffle=False, device=device):\n",
        "    prediction = ## make a forward pass for the model\n",
        "    reference = batch[target_column]\n",
        "\n",
        "    loss = compute_loss(reference, prediction)\n",
        "\n",
        "    val_loss += loss.item()\n",
        "    val_mae += compute_mae(reference, prediction).item()\n",
        "    val_batches += 1\n",
        "\n",
        "print(\"Final validation:\")\n",
        "print(\"val: loss={:.5f}, MAE={:.1f}\".format(val_loss / val_batches, val_mae / val_batches))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K82aWVWgonB_",
        "colab_type": "text"
      },
      "source": [
        "You should get **MAE ~= 7000**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azirw9VMonB_",
        "colab_type": "text"
      },
      "source": [
        "## Bonus: explaining model's predictions\n",
        "\n",
        "It's usually a good idea to understand what your model does before you let it make actual decisions. It's simple for linear models: just see which words learned positive or negative weights. However, its much harder for neural networks that learn complex nonlinear dependencies.\n",
        "\n",
        "There are, however, some ways to look inside the black box:\n",
        "* Seeing how model responds to input perturbations\n",
        "* Finding inputs that maximize/minimize activation of some chosen neurons (read more [on distill.pub](https://distill.pub/2018/building-blocks/))\n",
        "* Building local linear approximations to your neural network: [article](https://arxiv.org/abs/1602.04938), [eli5 library](https://github.com/TeamHG-Memex/eli5/tree/master/eli5/formatters)\n",
        "\n",
        "Today we gonna try the first method just because it's the simplest one.\n",
        "\n",
        "__Your task__ is to measure how does model prediction change if you replace certain tokens with UNKs. The core idea is that if dropping a word from text causes model to predict lower log-salary, than this word probably has positive contribution to salary (and vice versa)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAMHrE3ConB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def explain(model, sample, col_name='Title'):\n",
        "    \"\"\" Computes the effect each word had on model predictions \"\"\"\n",
        "    sample = dict(sample)\n",
        "    \n",
        "    sample_col_tokens = [tokens[token_to_id.get(tok, 0)] for tok in sample[col_name].split()]\n",
        "    data_drop_one_token = pd.DataFrame([sample] * (len(sample_col_tokens) + 1))\n",
        "\n",
        "    for drop_i in range(len(sample_col_tokens)):\n",
        "        data_drop_one_token.loc[drop_i, col_name] = ' '.join(\n",
        "            UNK if i == drop_i else tok for i, tok in enumerate(sample_col_tokens)\n",
        "        ) \n",
        "    batch = sample_batch(data_drop_one_token, None, device=device)\n",
        "\n",
        "    *predictions_drop_one_token, baseline_pred = model(\n",
        "        batch['Title'], batch['FullDescription'], batch['Categorical']\n",
        "    ).detach().to('cpu').numpy()\n",
        "    diffs = baseline_pred - predictions_drop_one_token\n",
        "    \n",
        "    return list(zip(sample_col_tokens, diffs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65BzyjyeonCC",
        "colab_type": "text"
      },
      "source": [
        "See some sample's weight:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zDcFJcDonCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = data.loc[np.random.randint(len(data))]\n",
        "print(\"Input:\", sample)\n",
        "\n",
        "tokens_and_weights = explain(model, sample, 'Title')\n",
        "print(tokens_and_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCsBFDLtonCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML, display_html\n",
        "\n",
        "def draw_html(\n",
        "    tokens_and_weights, cmap=plt.get_cmap(\"bwr\"), display=True,\n",
        "    token_template=\"\"\"<span style=\"background-color: {color_hex}\">{token}</span>\"\"\",\n",
        "    font_style=\"font-size:14px;\"\n",
        "):\n",
        "    \n",
        "    def get_color_hex(weight):\n",
        "        rgba = cmap(1.0 / (1 + np.exp(weight)), bytes=True)\n",
        "        return '#{:02X}{:02X}{:02X}'.format(*rgba[:3])\n",
        "    \n",
        "    tokens_html = [\n",
        "        token_template.format(token=token, color_hex=get_color_hex(weight))\n",
        "        for token, weight in tokens_and_weights\n",
        "    ]\n",
        "    \n",
        "    \n",
        "    raw_html = \"\"\"<p style=\"{}\">{}</p>\"\"\".format(font_style, ' '.join(tokens_html))\n",
        "    if display:\n",
        "        display_html(HTML(raw_html))\n",
        "        \n",
        "    return raw_html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xscA0l_ionCF",
        "colab_type": "text"
      },
      "source": [
        "Here `blue` – most important, `red` – least important:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g17A2RBonCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = np.random.randint(len(data))\n",
        "sample = data.loc[i]\n",
        "print(\"Index:\", i)\n",
        "\n",
        "# predict salary on sample\n",
        "sample_dict = dict(sample)\n",
        "sample_frame = pd.DataFrame([sample_dict])\n",
        "batch = sample_batch(sample_frame, None, device=device)\n",
        "\n",
        "prediction = model(batch['Title'], batch['FullDescription'], batch['Categorical'])\n",
        "predicted_salary = (torch.exp(prediction) - 1).item()\n",
        "print(\"Salary:\", predicted_salary)\n",
        "\n",
        "tokens_and_weights = explain(model, sample, 'Title')\n",
        "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
        "\n",
        "tokens_and_weights = explain(model, sample, 'FullDescription')\n",
        "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0EqJNjWonCI",
        "colab_type": "text"
      },
      "source": [
        "## Beast mode on!\n",
        "\n",
        "Here're some ideas, how to improve you validation MAE\n",
        "\n",
        "#### A) CNN architecture\n",
        "\n",
        "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
        "* Dropout\n",
        "* Batch Norm. This time it's `nn.BatchNorm1d`\n",
        "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels\n",
        "* More layers, more neurons, you know...\n",
        "\n",
        "\n",
        "#### B) Play with pooling\n",
        "\n",
        "There's more than one way to do max pooling:\n",
        "* Max over time is our `GlobalMaxPooling`\n",
        "* Average over time (excluding PAD)\n",
        "* Softmax-pooling:\n",
        "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
        "\n",
        "* Attentive pooling\n",
        "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
        "\n",
        ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
        "and $NN_{attn}$ is a small neural network\n",
        "\n",
        "\n",
        "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$\n",
        "\n",
        "#### C) Fun with embeddings\n",
        "\n",
        "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
        "\n",
        "* Use a pre-trained word2vec from [here](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) or [here](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/).\n",
        "* Start with pre-trained embeddings, then fine-tune them with gradient descent\n",
        "* Use the same embedding matrix in title and desc vectorizer\n",
        "\n",
        "#### D) Going recurrent\n",
        "\n",
        "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
        "\n",
        "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
        "  * Please bear in mind that while convolution uses [batch, units, time] dim order, \n",
        "    recurrent units are built for [batch, time, unit]. You may need to `torch.transpose`.\n",
        "\n",
        "* Since you know all the text in advance, use bidirectional RNN\n",
        "  * Run one LSTM from left to right\n",
        "  * Run another in parallel from right to left \n",
        "  * Concatenate their output sequences along unit axis (dim=-1)\n",
        "\n",
        "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
        "\n",
        "\n",
        "#### E) Optimizing seriously\n",
        "\n",
        "* You don't necessarily need 100 epochs. Use early stopping.\n",
        "  * In short, train until you notice that validation\n",
        "  * Maintain the best-on-validation snapshot via `model.state_dict`\n",
        "  * Plotting learning curves is usually a good idea"
      ]
    }
  ]
}