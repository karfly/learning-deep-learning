{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/karfly/learning-deep-learning/blob/master/09_transformer/postagging_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-tagging with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part-of-speech (POS) tagging** is the process of marking up a word in a text as corresponding to a particular part of speech (noun, verb, ...), based on both its definition and its context.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/pos-tagging.png\" width=480>\n",
    "\n",
    "Let's look at some basic examples:\n",
    "\n",
    "\n",
    "**Example 1.**\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{Input} &= [\\text{I love my cat}]\\\\\n",
    "\\text{Output} &= [\\text{PRON VERB PRON NOUN}]\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "**Example 2.**\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{Input} &= [\\text{august 11 , 2000}]\\\\\n",
    "\\text{Output} &= [\\text{PROPN NUM PUNCT NUM}]\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "---\n",
    "At first glance it seems that the POS-tagging problem can be solved by collecting a dictionary `{word : POS-tag}`. But natural language is very complex and usually a word's part of speech strongly depends on the context, like in the following examples: \n",
    "\n",
    "\n",
    "**Example 3.**\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{Input} &= [\\text{Look , there is a } \\textbf{bear}]\\\\\n",
    "\\text{Output} &= [\\text{VERB PUNCT DET VERB DET } \\textbf{NOUN}]\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "**Example 4.**\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{Input} &= [\\text{I can not } \\textbf{bear } \\text{it anymore}]\\\\\n",
    "\\text{Output} &= [\\text{PROPN VERB ADV } \\textbf{VERB } \\text{PROPN ADV}]\n",
    "\\end{align*}$$\n",
    "\n",
    "In the example 3 the word `bear` is a NOUN, but `bear` in the example 4 is a VERB. This can only be derived from the context. Such examples are not rare, on the contrary, they are very common in human languages (e.g. see [homonyms](https://en.wikipedia.org/wiki/Homonym)). Today we'll tackle this problem by using the most successful (so far) deep learning architecture for NLP – [Transformer](https://arxiv.org/abs/1706.03762)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework plan\n",
    "\n",
    "In this homework you'll implement and train 2 POS-tagging models: **Conv** (baseline) and **Transformer**. The **main goal** of this homework is to get to know with the **Transformer architecture** and to understand how it works from the inside.\n",
    "\n",
    "The plan for the homework is as follows:\n",
    "1. Load and prepare data (you'll use `UDPOS` dataset from [torchtext](https://pytorch.org/text/))\n",
    "2. Build baseline Conv model (already implemented for you)\n",
    "3. Implement training pipeline and train/evaluate baseline model\n",
    "4. Build Transformer (all fun is here!)\n",
    "5. Train/evaluate Transformer model (here you'll reuse the pipeline from pt. 3)\n",
    "6. Write a of short report\n",
    "\n",
    "## Grading (10 points total)\n",
    "The main metric in this homework is accuracy calculated on the **test** set (please, don't train on test set – it will be easily revealed by the TAs). Also, you are not allowed to change train/val/test data.\n",
    "\n",
    "The points are allocated to tasks as follows:\n",
    " 1. **[3 points]** Training and evaluation of baseline Conv model (don't change the model architecture!). The test accuracy must be **> 82.5%**\n",
    " 2. **[7 points]** Implementation, training and evaluation of Transformer model. The test accuracy must be **> 85%**\n",
    " \n",
    "Your homework submission should contain:\n",
    "1. Fully functional notebook, which has to reproduce all the reported results by clicking on the \"Run all\" button. The notebook must contain the filled in report (see the template in the very end of the notebook).\n",
    "2. Saved weights of the best models `ConvPOSTagger.best.pth` and `TransformerPOSTagger.best.pth` (saving is already implemented for you).\n",
    "3. The screenshots of accuracy curves from Tensorboard for both models on validation dataset.\n",
    "\n",
    "## Tips\n",
    "- Do not start writing code before you have read and understood the theory.\n",
    "- Read all the text in this notebook (hope, you're reading this).\n",
    "- Make one change at a time: never change several things at once (unless you are super confident). Train a model, introduce one change, train again.\n",
    "- Use Tensorboard. If you use [Google Colab](https://colab.research.google.com/) check this [hack](https://stackoverflow.com/a/57791702).\n",
    "- Pay much attention to accuracy and loss curves (e.g. in Tensorboard). Track failures early, stop bad experiments early.\n",
    "- Use GPU. If you don't have it, use [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    " \n",
    "## Disclaimer\n",
    "ConvNets can be better than Transformers in POS-tagging task in the case of small data. In this homework you'll train models on a very small dataset (12 543 train sentences) which is ridiculous on the scale of modern NLP datasets (millions or even billions of sentences!). Anyway, it's very important to understand how Transformers work and to be ready to apply them in a real-world problem with a large dataset, where Transformers outperform other architectures (Conv, RNN, ...) with a large margin.\n",
    "\n",
    "**Good luck and have fun!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, setup and handy functions\n",
    "If you don't have some library, just install it with `pip install ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchtext\n",
    "import torchtext.datasets\n",
    "import torchtext.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MAX_LENGTH = 200  # maximum length of the input sentence\n",
    "MIN_TOKEN_FREQUENCY = 10  # minimum occurrence frequency\n",
    "DO_TRAIN_CONV_POSTAGGER = True  # set False not to train Conv POS-tagger TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make experiments reproducible. Set random seed for `random`, `numpy`, `torch` and `cuda`/`cudnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handy function for setting up experiments with Tensorboard logging. By default all the Tensorboard logs will be stored in `./tb` dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment(title, logdir=\"./tb\"):\n",
    "    experiment_name = \"{}@{}\".format(title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "    writer = SummaryWriter(log_dir=os.path.join(logdir, experiment_name))\n",
    "    best_model_path = f\"{title}.best.pth\"\n",
    "    return writer, experiment_name, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you'll use `UDPOS` dataset for POS-tagging. In fact, `UDPOS` dataset is just a collection of pairs `(sequence of words : sequence of corresponding POS-tags)`.\n",
    "\n",
    "\n",
    "For easy access to the dataset, use [torchtext](https://pytorch.org/text/) library. At first glance, [torchtext](https://pytorch.org/text/)'s interfaces look very strange, but after multiple uses you get used to it, and it appears to be a very nice library. Let's define `fields` ([torchtext](https://pytorch.org/text/) abstraction) for text and POS-tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(\n",
    "    batch_first=True,\n",
    "    lower=True,  # make lowercase\n",
    "    init_token='<bos>', eos_token='<eos>'  # special tokens: beginning/end of sequence\n",
    ")\n",
    "\n",
    "POSTAG = torchtext.data.Field(\n",
    "    batch_first=True,\n",
    "    init_token='<bos>', eos_token='<eos>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get train/val/test data using defined fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = torchtext.datasets.UDPOS.splits(\n",
    "    fields=(('text', TEXT), ('postag', POSTAG))\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_data)} sequences\")\n",
    "print(f\"Val size: {len(val_data)} sequences\")\n",
    "print(f\"Test size: {len(test_data)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the vocabulary (*note:* using only train data) to get the mapping from token to some unique index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data.text, min_freq=MIN_TOKEN_FREQUENCY)  # filter out rarely occured tokens\n",
    "POSTAG.build_vocab(train_data.postag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert from token to unique index use `.vocab.stoi[...]` method (`.vocab.itos[...]` for inverse mapping). Here we collect unique indices of `padding` special tokens to use them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "POSTAG_PAD_IDX = POSTAG.vocab.stoi[POSTAG.pad_token]\n",
    "print(f\"TEXT_PAD_IDX={TEXT_PAD_IDX}, POSTAG_PAD_IDX={POSTAG_PAD_IDX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get vocabulary sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "OUTPUT_DIM = len(POSTAG.vocab)\n",
    "print(f\"Number of unique words: {INPUT_DIM}\")\n",
    "print(f\"Number of unique POS-tags: {OUTPUT_DIM}\")\n",
    "print(\"All POS-tags:\", ' '.join(POSTAG.vocab.freqs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the data iterators that we'll use for training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, val_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "     batch_size=BATCH_SIZE,\n",
    "     device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a batch and look inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "text, postag = batch.text, batch.postag\n",
    "\n",
    "print(f\"text.shape={text.shape} – [bs, seq_len]\")\n",
    "print(f\"postag.shape={postag.shape} – [bs, seq_len]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's all about the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv POS-tagger (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the baseline model, let's first formalize our task.\n",
    "\n",
    "POS-tagging is a \"sequence-to-sequence\" task, where input and output have the **same length**. Unlike, e.g. machine translation, where the output (translation) can be of any size, in POS-tagging input and output are **aligned**. The number of POS-tags is fixed and not large. So, formally speaking, POS-tagging is a classification problem, but the length of the input can vary.\n",
    "\n",
    "ConvNets are really good at this kind of task. So, let's implement some simple model consisting of Conv1d layers (with kernels of size 1 => no context is used). Here's the principal scheme:\n",
    "\n",
    "![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/conv-postagger.png)\n",
    "\n",
    "The main steps are as follows:\n",
    "1. Get embeddings of the input sequence\n",
    "2. Process the embedded sequence with $N$ Conv1d layers. The number of output channels of the final conv layer equals to the the number of unique POS-tags (`output_dim`)\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPOSTagger(nn.Module):\n",
    "    def __init__(self, n_tokens, hid_dim, output_dim):\n",
    "        \"\"\" \n",
    "        x -> Embedding -> Conv -> ReLU -> Conv -> ReLU -> Conv -> ReLU -> Conv\n",
    "        \n",
    "        n_tokens: # of unique tokens\n",
    "        hid_dim: # of channels of intermediate layers (with expansion)\n",
    "        output_dim: # of unique POS-tags)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(n_tokens, hid_dim)\n",
    "        \n",
    "        # here we use conv layer with kernel size = 1 as a baseline\n",
    "        # in fact, such model is identical to multiple linear layes applied to each embedded token\n",
    "        # you can try to increase the kernel size and see how good ConvNets are in POS-tagging when small amount of data is available\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(hid_dim, 2 * hid_dim, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(2 * hid_dim, 4 * hid_dim, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(4 * hid_dim, 8 * hid_dim, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(8 * hid_dim, output_dim, kernel_size=1)  # note: no activation here\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.long tensor of shape [batch_size, sequence_length]\n",
    "        returns: torch.float32 tensor of shape [batch_size, sequence_length, output_dim]\n",
    "        \"\"\"\n",
    "        x = self.emb(x)  # get embeddings\n",
    "        \n",
    "        # transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        \n",
    "        # encode\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        # transpose back to [batch, time, units]\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvPOSTagger(INPUT_DIM, 64, OUTPUT_DIM)\n",
    "dummy_input = ## your code here\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "print(f\"dummy_output.shape={dummy_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any loss function, suitable for classification, will also work for us, so we'll choose the obvious option – [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss) criterion. Make sure to ignore losses calculated over `<pad>` tokens, not to add noise to the total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=POSTAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's setup our model, optimizier and experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvPOSTagger(INPUT_DIM, 64, OUTPUT_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "writer, experiment_name, best_model_path = setup_experiment(model.__class__.__name__, logdir=\"./tb\")\n",
    "print(f\"Experiment name: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the total number of models' parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common practice in training deep learning models is to a create single-function `run_epoch(...)`, which can be used both for training and evaluation (see `phase` parameter). Here you need to implement forward of the model and the loss calculation. Good news is that you'll reuse this function later for training the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, iterator, optimizer, criterion, phase='train', epoch=0, writer=None):\n",
    "    is_train = (phase == 'train')\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    # variables for calculating accuracy\n",
    "    n_predicted = 0\n",
    "    n_true_predicted = 0\n",
    "    \n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for i, batch in enumerate(iterator):\n",
    "            global_i = len(iterator) * epoch + i\n",
    "            \n",
    "            # unpack batch\n",
    "            text, postag = batch.text, batch.postag\n",
    "            \n",
    "            # make prediction\n",
    "            ## your code here\n",
    "            \n",
    "            # reshape prediction to [-1, output_dim]\n",
    "            ## your code here\n",
    "            \n",
    "            # reshape gt labels to [-1, ]\n",
    "            ## your code here\n",
    "            \n",
    "            # calculate loss\n",
    "            ## your code here\n",
    "            \n",
    "            if is_train:\n",
    "                # make optimization step\n",
    "                ## your code here\n",
    "                \n",
    "            # calculate accuracy\n",
    "            n_true_predicted += torch.sum((pred.argmax(-1) == gt) * (gt != POSTAG_PAD_IDX)).item()  # don't calculate pad token\n",
    "            n_predicted += torch.sum(gt != POSTAG_PAD_IDX).item()\n",
    "            \n",
    "            # dump train metrics to tensorboard\n",
    "            if writer is not None and is_train:\n",
    "                writer.add_scalar(f\"loss/{phase}\", loss.item(), global_i)\n",
    "                \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # dump epoch metrics to tensorboard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(f\"loss_epoch/{phase}\", epoch_loss / len(iterator), epoch)\n",
    "            writer.add_scalar(f\"accuracy_epoch/{phase}\", n_true_predicted / n_predicted, epoch)\n",
    "\n",
    "        return epoch_loss / len(iterator), n_true_predicted / n_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's time to train the model. All logs are dumped to `Tesorboard`, so go there and enjoy your beautiful loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "if not DO_TRAIN_CONV_POSTAGGER:\n",
    "    n_epochs = 0\n",
    "\n",
    "best_val_loss = float('+inf')\n",
    "for epoch in range(n_epochs):    \n",
    "    train_loss, train_accuracy = run_epoch(model, train_iterator, optimizer, criterion, phase='train', epoch=epoch, writer=writer)\n",
    "    val_loss, val_accuracy = run_epoch(model, val_iterator, None, criterion, phase='val', epoch=epoch, writer=writer)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train accuracy: {train_accuracy * 100:.2f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. accuracy: {val_accuracy * 100:.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load our best model and evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_loss, test_accuracy = run_epoch(model, test_iterator, None, criterion, phase='val')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test accuracy: {test_accuracy:7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll implement functions to infer our model on any given sentence and display the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_model(sentence, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # get tokens\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token.lower() for token in sentence.split(' ')]  # split if string\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    \n",
    "    # add <bos> and <eos> special tokens to the beginning and to the end\n",
    "    tokens = ## your code here\n",
    "    \n",
    "    # convert tokens to indices\n",
    "    indices = ## your code here\n",
    "    indices = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # make predictions\n",
    "    with torch.no_grad():\n",
    "        pred = ## your code here\n",
    "        \n",
    "    # extract pos-tags indices from predictions using .argmax(...)\n",
    "    pred_postag_indices = ## your code here\n",
    "    \n",
    "    # convert from indices to pos-tags\n",
    "    pred_postags = ## your code here\n",
    "    \n",
    "    # cut off <bos> and <eos>\n",
    "    tokens = tokens[1:-1]\n",
    "    pred_postags = pred_postags[1:-1]\n",
    "\n",
    "    return tokens, pred_postags\n",
    "\n",
    "\n",
    "def print_predictions(tokens, pred_postags, gt_postags=None):\n",
    "    print(\"===> Input sentence:\", ' '.join(tokens))\n",
    "    print()\n",
    "    \n",
    "    if gt_postags is not None:\n",
    "        print(\"Pred. POS-tag\\tGT POS-tag\\tCorrect?\\tToken\\n\")\n",
    "        for token, pred_postag, gt_postag in zip(tokens, pred_postags, gt_postags):\n",
    "            correct = '✔' if pred_postag == gt_postag else '✘'\n",
    "            print(f\"{pred_postag}\\t\\t{gt_postag}\\t\\t{correct}\\t\\t{token}\")\n",
    "    else:\n",
    "        print(\"Pred. POS-tag\\tToken\\n\")\n",
    "\n",
    "        for token, pred_postag in zip(tokens, pred_postags):\n",
    "            print(f\"{pred_postag}\\t\\t{token}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = np.random.randint(0, len(test_data.examples))\n",
    "\n",
    "sentence = test_data.examples[example_index].text\n",
    "gt_postags = test_data.examples[example_index].postag\n",
    "tokens, pred_postags = infer_model(sentence, model, device)\n",
    "\n",
    "print_predictions(tokens, pred_postags, gt_postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can try to POS-tag our own sentences. **Likely** the baseline model will fail in the third sentence and tag `bear` as a `NOUN`, not `VERB`. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I will definitely pass this homework'\n",
    "print_predictions(*infer_model(sentence, model, device))\n",
    "\n",
    "sentence = 'Look , there is a bear'\n",
    "print_predictions(*infer_model(sentence, model, device))\n",
    "\n",
    "sentence = 'I can not bear it anymore'\n",
    "print_predictions(*infer_model(sentence, model, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7 points) Transformer\n",
    "\n",
    "In this part you will implement a slightly modified version of the Transformer model from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. Actually, you'll only implement the encoder part, because it's enough for POS-tagging task.\n",
    "\n",
    "It's **highly recommended** to read [the original Transformer paper](https://arxiv.org/abs/1706.03762) or [this nice article](https://jalammar.github.io/illustrated-transformer/) before starting coding.\n",
    "\n",
    "The Transformer consists of multiple nested modules like a matryoshka (e.g. `Encoder` consists of `EncoderLayer`, which consists of `MultiHeadAttentionLayer` and `PositionwiseFeedforwardLayer`). In this notebook the implementation of modules is ordered in a [top-down](https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design) manner – first the higher-level modules are implemented (e.g. `Encoder`), that use lower-level modules (e.g. `EncoderLayer`), which are not implemented yet. It's **strongly recommended** to read the notebook to the end to be aware of the code structure, before you start implementing the Transformer.\n",
    "\n",
    "![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer.png)\n",
    "\n",
    "Similar to the Conv model, the Transformer does not use any recurrence. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, **attention mechanisms** and normalizations. \n",
    "\n",
    "As of May 2020, Transformers are the dominant architecture in NLP and are used to achieve state-of-the-art results for many tasks and it appears as if they will be for the near future. \n",
    "\n",
    "\n",
    "*Note:* in this notebook you'll implement a **learned positional encoding** (in the fashion of [BERT](https://arxiv.org/abs/1810.04805)), not the static one from the [original paper](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The Transformer's encoder attempts to *transform* the entire source sentence, $X = (x_1, ... ,x_n)$, into a sequence of context vectors, $Z = (z_1, ... , z_n)$. So, if our input sequence was 5 tokens long we would have $Z = (z_1, z_2, z_3, z_4, z_5)$. Why do we call this a sequence of context vectors and not a sequence of hidden states? A hidden state at time $t$ in an RNN has only seen tokens $x_t$ and all the tokens before it. However, each context vector here has seen all tokens at all positions within the input sequence.\n",
    "\n",
    "![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer-encoder.png)\n",
    "\n",
    "First, the tokens are passed through a standard embedding layer. Next, as the model has no recurrence it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence, starting with the first token, the `<bos>` (beginning of sequence) token, in position 0. The position embedding has a \"vocabulary\" size of `max_length`, which means our model can accept sentences up to `max_length` tokens long.\n",
    "\n",
    "The original Transformer implementation from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper does not learn positional embeddings. Instead it uses a fixed static embedding. Modern Transformer architectures, like [BERT](https://arxiv.org/abs/1810.04805), use positional embeddings instead, so we'll use them.\n",
    "\n",
    "Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position within the sequence. However, before they are summed, the token embeddings are multiplied by a scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, `hid_dim`. This (supposedly) reduces variance in the embeddings and the model is difficult to train reliably without this scaling factor. Dropout is then applied to the combined embeddings.\n",
    "\n",
    "The combined embeddings are then passed through $N$ *encoder layers* to get $Z$, which is the output and can be used for any downstream task (e.g. POS-tagging).\n",
    "\n",
    "The mask, `mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a `<pad>` token and 0 when it is a `<pad>` token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to `<pad>` tokens, which contain no useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 padding_index=None,\n",
    "                 max_length=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.padding_index = padding_index  # if None, don't use masking\n",
    "        \n",
    "        # embeddings\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        # encoder layers (implemented below)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        \n",
    "        # dropout is applied after summing up token and positional embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # scale parameter\n",
    "        self.scale = torch.nn.Parameter(torch.sqrt(torch.tensor(hid_dim, dtype=torch.float32)))\n",
    "        \n",
    "        # custom weight initialization\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x (batch of token indices): torch.long tensor of shape [bs, seq_len]\n",
    "        \n",
    "        returns (encoded sequence): torch.float32 tensor of shape [bs, seq_len, output_dim]\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        bs, seq_len = x.shape[:2]\n",
    "        \n",
    "        # compute non-padding mask (use self.padding_index)\n",
    "        mask = None\n",
    "        if self.padding_index is not None:\n",
    "            mask = ## your code here\n",
    "        \n",
    "        # get token embeddings and scale with self.scale parameter\n",
    "        ## your code here\n",
    "        \n",
    "        # generate input [0, 1, ..., seq_len - 1] for positional embedder [bs, seq_len]\n",
    "        ## your code here\n",
    "        \n",
    "        # get pos embeddings\n",
    "        ## your code here\n",
    "        \n",
    "        # sum up token and positional embeddings\n",
    "        ## your code here\n",
    "        \n",
    "        # apply dropout\n",
    "        ## your code here\n",
    "        \n",
    "        # apply encoder layers one by one; input shape is [bs, seq_len, hid dim]\n",
    "        ## your code here\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "The encoder layers are where all of the \"meat\" of the encoder is contained.\n",
    "\n",
    "![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer-encoder-layer.png)\n",
    "\n",
    "The encoder layer consists of 2 main blocks:\n",
    "1. Pass the source sentence and its mask into the *multi-head attention layer*, perform dropout on it, add a residual connection and pass it through a [Layer Normalization](https://arxiv.org/abs/1607.06450) layer.\n",
    "2. Pass the output of the 1st block through a *position-wise feedforward* layer and then, again, apply dropout, a residual connection and then layer normalization to get the output of this layer which is fed into the next layer (the parameters are not shared between layers)\n",
    "\n",
    "\n",
    "The multi-head attention layer is used by the encoder layer to attend to the source sentence, i.e. it is calculating and applying attention over itself instead of another sequence, hence we call it *self attention*.\n",
    "\n",
    "[This article](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/) goes into more detail about layer normalization, but the gist is that it normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self-attention layer normalization\n",
    "        self.attention_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        # positionwise feedforward layer normalization\n",
    "        self.pf_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        # attention layer (implemented below)\n",
    "        self.attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
    "        \n",
    "        # positionwise feedforward layer (implemented below)\n",
    "        self.pf = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        \n",
    "        # dropout is applied to the outputs of the attention and positionwise feedforward layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x (sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n",
    "        mask (mask of valid elements): torch.bool tensor of shape [bs, seq_len]\n",
    "        \n",
    "        returns (processed sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n",
    "        \"\"\"\n",
    "        ### block 1\n",
    "        # calculate self-attention + dropout\n",
    "        ## your code here\n",
    "        \n",
    "        # residual (attention) + attention layer norm\n",
    "        ## your code here\n",
    "        \n",
    "        ### block 2\n",
    "        # calculate positionwise feedforward + dropout\n",
    "        ## your code here\n",
    "        \n",
    "        # residual (positionwise feedforward) + positionwise feedforward layer norm\n",
    "        ## your code here\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Layer\n",
    "\n",
    "One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*. \n",
    "\n",
    "![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer-attention.png)\n",
    "\n",
    "Attention can be thought of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
    "\n",
    "The Transformer uses *scaled dot-product attention*, where the query and the key are combined by taking the dot product between them, then applying the softmax operation, scaling by $d_k$ and finally multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n",
    "\n",
    "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
    "\n",
    "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
    "\n",
    "The steps in this module are as follows:\n",
    "1. Calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v` to get `Q`, `K` and `V`.\n",
    "2. Split the `hid_dim` of the query, key and value into `n_heads` (use `.view`) and correctly permute them so they can be multiplied together.\n",
    "3. Calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calculated as `hid_dim // n_heads`. Mask the energy not to pay attention over any elements of the sequence we shouldn't.\n",
    "4. Apply the softmax, dropout and then apply the attention to the value heads, `V`, before combining the `n_heads` together.\n",
    "5. Finally, multiply the output of step 4 with $W^O$ (represented as `fc_o`). \n",
    "\n",
    "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` (or `@` operator) which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n",
    "\n",
    "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0, \"hid_dim must be divisible by n_heads\"\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        # query, key and value linear networks\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        # output linear networks\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        # dropout is applied to attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # scale parameter\n",
    "        self.scale = torch.nn.Parameter(torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)))\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query/key/value (batch of queries/keys/values): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n",
    "        mask (mask of valid elements): torch.bool tensor of shape [bs, seq_len]\n",
    "        \n",
    "        returns (multi-head attention): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        bs = query.shape[0]\n",
    "        \n",
    "        # calculate Q, K, V using corresponding linear networks\n",
    "        q, k, v = self.fc_q(query), self.fc_k(key), self.fc_v(value)  # shape is [bs, seq_len, hid_dim]\n",
    "                \n",
    "        # prepare Q, K, V for .matmul() or `@` operator\n",
    "        # shape is [bs, n_heads, seq_len, head_dim]\n",
    "        ## your code here\n",
    "        \n",
    "        # compute energy using .matmul() or `@` operator (don't forget to scale!)\n",
    "        # shape is [bs, n_heads, seq_len, seq_len]\n",
    "        energy = ## your code here  \n",
    "        \n",
    "        # apply mask – 1 in mask is a valid element, 0 - not (use .masked_fill())\n",
    "        if mask is not None:\n",
    "            energy = ## your code here\n",
    "        \n",
    "        # apply softmax along the last dim of energy and get the attention weights\n",
    "        # shape is [bs, n_heads, seq_len]\n",
    "        attention = ## your code here\n",
    "        \n",
    "        # weight values with calculated attention (use .matmul() or `@` operator)\n",
    "        # shape is [bs, n_heads, seq_len, head_dim]\n",
    "        x = ## your code here\n",
    "        \n",
    "        # squash 1 and 4 dims back\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(bs, -1, self.hid_dim)  # shape is [bs, seq_len, hid_dim]\n",
    "        \n",
    "        # apply output linear layer\n",
    "        x = ## your code here\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer\n",
    "\n",
    "The other main block inside the encoder layer is the *position-wise feedforward layer* This is relatively simple compared to the multi-head attention layer. The input is transformed from `hid_dim` to `pf_dim`, where `pf_dim` is usually a lot larger than `hid_dim`. The original Transformer used a `hid_dim` of 512 and a `pf_dim` of 2048. The ReLU activation function and dropout are applied before it is transformed back into a `hid_dim` representation. \n",
    "\n",
    "Why is this used? Unfortunately, it is never explained in the paper.\n",
    "\n",
    "*The bonus*: we implemented this layer for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # linear layers\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        # dropout is applied after the first layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x (sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n",
    "        \n",
    "        returns (processed sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n",
    "        \"\"\"\n",
    "        # apply linear layers + dropout\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer POS-tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the parts of the transformer and we can build a Transformer-based POS-tagger. It consists of Transformer encoder and single linear layer, which predicts the classes of POS-tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPOSTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hid_dim=64, \n",
    "                 n_layers=8, \n",
    "                 n_heads=8, \n",
    "                 pf_dim=64,\n",
    "                 dropout=0.1, \n",
    "                 padding_index=None,\n",
    "                 max_length=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.encoder = Encoder(\n",
    "            input_dim,\n",
    "            hid_dim=hid_dim, \n",
    "            n_layers=n_layers, \n",
    "            n_heads=n_heads, \n",
    "            pf_dim=pf_dim,\n",
    "            dropout=dropout, \n",
    "            padding_index=padding_index,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        # linear layer to predict (classify) pos-tags\n",
    "        self.postag_predictor = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply encoder\n",
    "        ## your code here\n",
    "        \n",
    "        # predict postags\n",
    "        ## your code here\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Transformer Model\n",
    "\n",
    "The training pipeline for the Transformer model is absolutely the same except hyperparameters. We encourage you to tune them and find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_dim = ## your code here\n",
    "n_layers = ## your code here\n",
    "n_heads = ## your code here\n",
    "pf_dim = ## your code here\n",
    "dropout = ## your code here\n",
    "\n",
    "model = TransformerPOSTagger(\n",
    "    INPUT_DIM,\n",
    "    OUTPUT_DIM,\n",
    "    hid_dim=hid_dim, \n",
    "    n_layers=n_layers, \n",
    "    n_heads=n_layers, \n",
    "    pf_dim=pf_dim, \n",
    "    dropout=dropout, \n",
    "    padding_index=TEXT_PAD_IDX,\n",
    "    max_length=MAX_LENGTH\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate needs to be lower than the default used by Adam, otherwise the learning can be unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ## your code here\n",
    "optimizer = ## your code here\n",
    "\n",
    "writer, experiment_name, best_model_path = setup_experiment(model.__class__.__name__, logdir=\"./tb\")\n",
    "print(f\"Experiment name: {experiment_name}\")\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Transformer POS-tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = ## your code here\n",
    "\n",
    "best_val_loss = float('+inf')\n",
    "for epoch in range(n_epochs):    \n",
    "    train_loss, train_accuracy = run_epoch(model, train_iterator, optimizer, criterion, phase='train', epoch=epoch, writer=writer)\n",
    "    val_loss, val_accuracy = run_epoch(model, val_iterator, None, criterion, phase='val', epoch=epoch, writer=writer)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train accuracy: {train_accuracy * 100:.2f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. accuracy: {val_accuracy * 100:.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the final test accuracy and save best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_loss, test_accuracy = run_epoch(model, test_iterator, None, criterion, phase='val')\n",
    "print(f'Test Loss: {test_loss:.3f} | Test accuracy: {test_accuracy:7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore how the trained model works on test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = np.random.randint(0, len(test_data.examples))\n",
    "\n",
    "sentence = test_data.examples[example_index].text\n",
    "gt_postags = test_data.examples[example_index].postag\n",
    "tokens, pred_postags = infer_model(sentence, model, device)\n",
    "\n",
    "print_predictions(tokens, pred_postags, gt_postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on our own sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I will definitely pass this homework'\n",
    "print_predictions(*infer_model(sentence, model, device))\n",
    "\n",
    "sentence = 'Look , there is a bear'\n",
    "print_predictions(*infer_model(sentence, model, device))\n",
    "\n",
    "sentence = 'I can not bear it anymore'\n",
    "print_predictions(*infer_model(sentence, model, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "1. Did you manage to train the baseline Conv model with the test accuracy **> 82.5%**? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n",
    "\n",
    "2. Did you manage to train the Transformer model with the test accuracy **> 85.0%**? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n",
    "\n",
    "3. Did your Trasformer managed to correctly tag `bear` as a `VERB` in sentence **\"I can not bear it anymore\"**? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n",
    "\n",
    "4. Did you attach to your submission the screenshots of accuracy curves from Tensorboard for both models on validation dataset? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n",
    "\n",
    "5. Did you attach to your submission the model checkpoints: `ConvPOSTagger.best.pth` and `TransformerPOSTagger.best.pth`? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n",
    "\n",
    "6. What was the most difficult part of the homework for you? [Your answer here]\n",
    "\n",
    "7. What experiments with Transformer did you conduct? What hyperparameters worked the best? [Your answer here]\n",
    "\n",
    "8. And here you're free to tell us everything you want. [Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "- Big thanks to Ben Trevett for creating the core part of the Transformer code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
